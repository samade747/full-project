{"allContent":{"docusaurus-plugin-css-cascade-layers":{},"docusaurus-plugin-content-docs":{"default":{"loadedVersions":[{"versionName":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","path":"/full-project/docs","tagsPath":"/full-project/docs/tags","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs","isLast":true,"routePriority":-1,"sidebarFilePath":"D:\\github\\full-project\\sidebars.ts","contentPath":"D:\\github\\full-project\\docs","docs":[{"id":"hardware/conclusion/index","title":"Conclusion","description":"Setting up the right hardware is a critical first step in any robotics project. By choosing the right workstation, leveraging the power of edge computing, selecting the appropriate sensors and actuators, and outfitting your lab with the necessary infrastructure, you will be well on your way to building the next generation of intelligent robots.","source":"@site/docs/hardware/conclusion/index.md","sourceDirName":"hardware/conclusion","slug":"/hardware/conclusion/","permalink":"/full-project/docs/hardware/conclusion/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/conclusion/index.md","tags":[],"version":"current","frontMatter":{"title":"Conclusion"},"sidebar":"tutorialSidebar","previous":{"title":"Lab Infrastructure","permalink":"/full-project/docs/hardware/lab-infrastructure"}},{"id":"hardware/edge-computing-jetson","title":"Edge Computing and the NVIDIA Jetson","description":"While your workstation is where you will do most of your development, the code you write will ultimately run on a robot. For many modern robotics applications, this means running AI models and other computationally intensive tasks on an \"edge\" deviceâ€”a small, low-power computer that is embedded on the robot itself.","source":"@site/docs/hardware/edge-computing-jetson.md","sourceDirName":"hardware","slug":"/hardware/edge-computing-jetson","permalink":"/full-project/docs/hardware/edge-computing-jetson","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/edge-computing-jetson.md","tags":[],"version":"current","frontMatter":{"title":"Edge Computing and the NVIDIA Jetson"},"sidebar":"tutorialSidebar","previous":{"title":"Workstation Requirements","permalink":"/full-project/docs/hardware/workstation-requirements"},"next":{"title":"Sensors and Actuators","permalink":"/full-project/docs/hardware/sensors-actuators"}},{"id":"hardware/index","title":"Hardware Setup","description":"Building Your Robotics Lab","source":"@site/docs/hardware/index.md","sourceDirName":"hardware","slug":"/hardware/","permalink":"/full-project/docs/hardware/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/index.md","tags":[],"version":"current","frontMatter":{"title":"Hardware Setup"}},{"id":"hardware/introduction/index","title":"Building Your Robotics Lab","description":"A successful robotics project starts with a solid hardware foundation. Whether you are a student, a researcher, or a hobbyist, having the right hardware and a well-equipped workspace is essential for bringing your robotic creations to life.","source":"@site/docs/hardware/introduction/index.md","sourceDirName":"hardware/introduction","slug":"/hardware/introduction/","permalink":"/full-project/docs/hardware/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"Building Your Robotics Lab"},"sidebar":"tutorialSidebar","previous":{"title":"7. Hardware Setup","permalink":"/full-project/docs/category/7-hardware-setup"},"next":{"title":"Workstation Requirements","permalink":"/full-project/docs/hardware/workstation-requirements"}},{"id":"hardware/lab-infrastructure","title":"Lab Infrastructure","description":"In addition to the robot itself, you will need a well-equipped lab space to support your robotics development.","source":"@site/docs/hardware/lab-infrastructure.md","sourceDirName":"hardware","slug":"/hardware/lab-infrastructure","permalink":"/full-project/docs/hardware/lab-infrastructure","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/lab-infrastructure.md","tags":[],"version":"current","frontMatter":{"title":"Lab Infrastructure"},"sidebar":"tutorialSidebar","previous":{"title":"Sensors and Actuators","permalink":"/full-project/docs/hardware/sensors-actuators"},"next":{"title":"Conclusion","permalink":"/full-project/docs/hardware/conclusion/"}},{"id":"hardware/sensors-actuators","title":"Sensors and Actuators","description":"Sensors: The Senses of the Robot","source":"@site/docs/hardware/sensors-actuators.md","sourceDirName":"hardware","slug":"/hardware/sensors-actuators","permalink":"/full-project/docs/hardware/sensors-actuators","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/sensors-actuators.md","tags":[],"version":"current","frontMatter":{"title":"Sensors and Actuators"},"sidebar":"tutorialSidebar","previous":{"title":"Edge Computing and the NVIDIA Jetson","permalink":"/full-project/docs/hardware/edge-computing-jetson"},"next":{"title":"Lab Infrastructure","permalink":"/full-project/docs/hardware/lab-infrastructure"}},{"id":"hardware/workstation-requirements","title":"Workstation Requirements","description":"Your workstation is the command center of your robotics development. It is where you will write code, run simulations, and analyze data. The requirements for a robotics workstation can vary depending on the complexity of your projects, but here are some general guidelines:","source":"@site/docs/hardware/workstation-requirements.md","sourceDirName":"hardware","slug":"/hardware/workstation-requirements","permalink":"/full-project/docs/hardware/workstation-requirements","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/hardware/workstation-requirements.md","tags":[],"version":"current","frontMatter":{"title":"Workstation Requirements"},"sidebar":"tutorialSidebar","previous":{"title":"Building Your Robotics Lab","permalink":"/full-project/docs/hardware/introduction/"},"next":{"title":"Edge Computing and the NVIDIA Jetson","permalink":"/full-project/docs/hardware/edge-computing-jetson"}},{"id":"humanoid-robotics/bipedal-locomotion","title":"Bipedal Locomotion: The Art of Walking","description":"Walking on two legs is a remarkably complex task that humans perform with ease, but it is one of the biggest challenges in humanoid robotics. Bipedal locomotion requires a delicate balance of control, perception, and planning.","source":"@site/docs/humanoid-robotics/bipedal-locomotion.md","sourceDirName":"humanoid-robotics","slug":"/humanoid-robotics/bipedal-locomotion","permalink":"/full-project/docs/humanoid-robotics/bipedal-locomotion","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/bipedal-locomotion.md","tags":[],"version":"current","frontMatter":{"id":"bipedal-locomotion","title":"Bipedal Locomotion: The Art of Walking"},"sidebar":"tutorialSidebar","previous":{"title":"Kinematics and Dynamics","permalink":"/full-project/docs/humanoid-robotics/kinematics-dynamics"},"next":{"title":"Manipulation and Grasping","permalink":"/full-project/docs/humanoid-robotics/manipulation-grasping"}},{"id":"humanoid-robotics/conclusion/index","title":"Conclusion","description":"Humanoid robotics is a field of immense challenge and opportunity. By bringing together advances in mechanics, control, perception, and AI, we are getting closer to creating robots that can walk, talk, and work alongside us in our daily lives. The journey is long, but the potential rewards are enormous.","source":"@site/docs/humanoid-robotics/conclusion/index.md","sourceDirName":"humanoid-robotics/conclusion","slug":"/humanoid-robotics/conclusion/","permalink":"/full-project/docs/humanoid-robotics/conclusion/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/conclusion/index.md","tags":[],"version":"current","frontMatter":{"title":"Conclusion"},"sidebar":"tutorialSidebar","previous":{"title":"Human-Robot Interaction (HRI)","permalink":"/full-project/docs/humanoid-robotics/human-robot-interaction"},"next":{"title":"7. Hardware Setup","permalink":"/full-project/docs/category/7-hardware-setup"}},{"id":"humanoid-robotics/human-robot-interaction","title":"Human-Robot Interaction (HRI)","description":"As humanoid robots become more prevalent in our daily lives, it is crucial that they can interact with humans in a safe, natural, and intuitive way. This is the domain of Human-Robot Interaction (HRI).","source":"@site/docs/humanoid-robotics/human-robot-interaction.md","sourceDirName":"humanoid-robotics","slug":"/humanoid-robotics/human-robot-interaction","permalink":"/full-project/docs/humanoid-robotics/human-robot-interaction","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/human-robot-interaction.md","tags":[],"version":"current","frontMatter":{"title":"Human-Robot Interaction (HRI)"},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation and Grasping","permalink":"/full-project/docs/humanoid-robotics/manipulation-grasping"},"next":{"title":"Conclusion","permalink":"/full-project/docs/humanoid-robotics/conclusion/"}},{"id":"humanoid-robotics/index","title":"Humanoid Robotics","description":"The Grand Challenge of Humanoid Robotics","source":"@site/docs/humanoid-robotics/index.md","sourceDirName":"humanoid-robotics","slug":"/humanoid-robotics/","permalink":"/full-project/docs/humanoid-robotics/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/index.md","tags":[],"version":"current","frontMatter":{"title":"Humanoid Robotics"}},{"id":"humanoid-robotics/introduction/index","title":"The Grand Challenge of Humanoid Robotics","description":"Humanoid robots, with their human-like form and capabilities, represent one of the grand challenges of robotics. The goal is to create machines that can operate in human-centric environments, use human tools, and interact with humans in a natural and intuitive way.","source":"@site/docs/humanoid-robotics/introduction/index.md","sourceDirName":"humanoid-robotics/introduction","slug":"/humanoid-robotics/introduction/","permalink":"/full-project/docs/humanoid-robotics/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"The Grand Challenge of Humanoid Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"6. Humanoid Robotics","permalink":"/full-project/docs/category/6-humanoid-robotics"},"next":{"title":"Kinematics and Dynamics","permalink":"/full-project/docs/humanoid-robotics/kinematics-dynamics"}},{"id":"humanoid-robotics/kinematics-dynamics","title":"Kinematics and Dynamics","description":"The motion of a humanoid robot is governed by the principles of kinematics and dynamics.","source":"@site/docs/humanoid-robotics/kinematics-dynamics.md","sourceDirName":"humanoid-robotics","slug":"/humanoid-robotics/kinematics-dynamics","permalink":"/full-project/docs/humanoid-robotics/kinematics-dynamics","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/kinematics-dynamics.md","tags":[],"version":"current","frontMatter":{"title":"Kinematics and Dynamics"},"sidebar":"tutorialSidebar","previous":{"title":"The Grand Challenge of Humanoid Robotics","permalink":"/full-project/docs/humanoid-robotics/introduction/"},"next":{"title":"Bipedal Locomotion: The Art of Walking","permalink":"/full-project/docs/humanoid-robotics/bipedal-locomotion"}},{"id":"humanoid-robotics/manipulation-grasping","title":"Manipulation and Grasping","description":"For a humanoid robot to be truly useful, it must be able to manipulate objects in its environment. This involves a combination of perception, planning, and control.","source":"@site/docs/humanoid-robotics/manipulation-grasping.md","sourceDirName":"humanoid-robotics","slug":"/humanoid-robotics/manipulation-grasping","permalink":"/full-project/docs/humanoid-robotics/manipulation-grasping","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/humanoid-robotics/manipulation-grasping.md","tags":[],"version":"current","frontMatter":{"title":"Manipulation and Grasping"},"sidebar":"tutorialSidebar","previous":{"title":"Bipedal Locomotion: The Art of Walking","permalink":"/full-project/docs/humanoid-robotics/bipedal-locomotion"},"next":{"title":"Human-Robot Interaction (HRI)","permalink":"/full-project/docs/humanoid-robotics/human-robot-interaction"}},{"id":"intro","title":"Introduction Of Book","description":"Welcome to the ultimate guide to embodied intelligence!","source":"@site/docs/intro.md","sourceDirName":".","slug":"/intro","permalink":"/full-project/docs/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Introduction Of Book"},"sidebar":"tutorialSidebar","next":{"title":"1. Physical AI Fundamentals","permalink":"/full-project/docs/category/physical-ai"}},{"id":"physical-ai/conclusion/index","title":"Conclusion","description":"Physical AI is a rapidly advancing field with the potential to revolutionize our world. By creating intelligent agents that can operate in the physical world, we can automate dangerous and tedious tasks, assist the elderly and disabled, and explore new frontiers in science and space.","source":"@site/docs/physical-ai/conclusion/index.md","sourceDirName":"physical-ai/conclusion","slug":"/physical-ai/conclusion/","permalink":"/full-project/docs/physical-ai/conclusion/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/conclusion/index.md","tags":[],"version":"current","frontMatter":{"title":"Conclusion"},"sidebar":"tutorialSidebar","previous":{"title":"The Digital-to-Physical Transition","permalink":"/full-project/docs/physical-ai/digital-to-physical-transition/"},"next":{"title":"2. ROS 2 (Robot Operating System)","permalink":"/full-project/docs/category/2-ros-2-robot-operating-system"}},{"id":"physical-ai/core-principles/index","title":"Core Principles of Embodied Intelligence","description":"The core tenet of Embodied Intelligence is that intelligence is not an abstract property of a disembodied mind, but rather an emergent property of an agent's physical interactions with its environment. The body is not just a vessel for the brain; it is an integral part of the cognitive process.","source":"@site/docs/physical-ai/core-principles/index.md","sourceDirName":"physical-ai/core-principles","slug":"/physical-ai/core-principles/","permalink":"/full-project/docs/physical-ai/core-principles/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/core-principles/index.md","tags":[],"version":"current","frontMatter":{"title":"Core Principles of Embodied Intelligence"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Physical AI","permalink":"/full-project/docs/physical-ai/introduction/"},"next":{"title":"The Digital-to-Physical Transition","permalink":"/full-project/docs/physical-ai/digital-to-physical-transition/"}},{"id":"physical-ai/digital-to-physical-transition/index","title":"The Digital-to-Physical Transition","description":"Bridging the gap between the digital and physical worlds is one of the biggest challenges in Physical AI. The real world is far more complex and unpredictable than any simulation.","source":"@site/docs/physical-ai/digital-to-physical-transition/index.md","sourceDirName":"physical-ai/digital-to-physical-transition","slug":"/physical-ai/digital-to-physical-transition/","permalink":"/full-project/docs/physical-ai/digital-to-physical-transition/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/digital-to-physical-transition/index.md","tags":[],"version":"current","frontMatter":{"title":"The Digital-to-Physical Transition"},"sidebar":"tutorialSidebar","previous":{"title":"Core Principles of Embodied Intelligence","permalink":"/full-project/docs/physical-ai/core-principles/"},"next":{"title":"Conclusion","permalink":"/full-project/docs/physical-ai/conclusion/"}},{"id":"physical-ai/index","title":"Physical AI Fundamentals","description":"Introduction to Physical AI","source":"@site/docs/physical-ai/index.md","sourceDirName":"physical-ai","slug":"/physical-ai/","permalink":"/full-project/docs/physical-ai/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/index.md","tags":[],"version":"current","frontMatter":{"title":"Physical AI Fundamentals"}},{"id":"physical-ai/introduction/index","title":"Introduction to Physical AI","description":"Physical AI, often used interchangeably with Embodied AI, represents a paradigm shift in the field of artificial intelligence. It moves beyond the confines of digital computation and virtual environments to create intelligent systems that can perceive, reason, and act in the physical world. This is the domain of robots that walk, drones that fly, and autonomous cars that navigate our streets.","source":"@site/docs/physical-ai/introduction/index.md","sourceDirName":"physical-ai/introduction","slug":"/physical-ai/introduction/","permalink":"/full-project/docs/physical-ai/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"Introduction to Physical AI"},"sidebar":"tutorialSidebar","previous":{"title":"1. Physical AI Fundamentals","permalink":"/full-project/docs/category/physical-ai"},"next":{"title":"Core Principles of Embodied Intelligence","permalink":"/full-project/docs/physical-ai/core-principles/"}},{"id":"ros2/communication-patterns/index","title":"Communication Patterns","description":"ROS 2 provides several communication patterns for nodes to exchange data.","source":"@site/docs/ros2/communication-patterns/index.md","sourceDirName":"ros2/communication-patterns","slug":"/ros2/communication-patterns/","permalink":"/full-project/docs/ros2/communication-patterns/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/communication-patterns/index.md","tags":[],"version":"current","frontMatter":{"title":"Communication Patterns"},"sidebar":"tutorialSidebar","previous":{"title":"Core Architecture","permalink":"/full-project/docs/ros2/core-architecture/"},"next":{"title":"Python Integration (rclpy)","permalink":"/full-project/docs/ros2/python-integration-rclpy/"}},{"id":"ros2/conclusion/index","title":"Conclusion","description":"ROS 2 is a powerful and flexible framework for robotics development. Its modular architecture, robust communication patterns, and strong community support make it an ideal choice for a wide range of robotics applications, from simple hobbyist projects to complex industrial and commercial systems. This chapter has provided a solid foundation for understanding the core concepts of ROS 2. In the following chapters, we will dive deeper into each of these topics and learn how to use them to build our own robotic systems.","source":"@site/docs/ros2/conclusion/index.md","sourceDirName":"ros2/conclusion","slug":"/ros2/conclusion/","permalink":"/full-project/docs/ros2/conclusion/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/conclusion/index.md","tags":[],"version":"current","frontMatter":{"title":"Conclusion"},"sidebar":"tutorialSidebar","previous":{"title":"URDF (Unified Robot Description Format)","permalink":"/full-project/docs/ros2/urdf/"},"next":{"title":"3. Robot Simulation","permalink":"/full-project/docs/category/3-robot-simulation"}},{"id":"ros2/core-architecture/index","title":"Core Architecture","description":"At its heart, ROS 2 is a distributed system of processes (called \"nodes\") that communicate with each other to perform complex tasks.","source":"@site/docs/ros2/core-architecture/index.md","sourceDirName":"ros2/core-architecture","slug":"/ros2/core-architecture/","permalink":"/full-project/docs/ros2/core-architecture/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/core-architecture/index.md","tags":[],"version":"current","frontMatter":{"title":"Core Architecture"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to ROS 2","permalink":"/full-project/docs/ros2/introduction/"},"next":{"title":"Communication Patterns","permalink":"/full-project/docs/ros2/communication-patterns/"}},{"id":"ros2/index","title":"ROS 2 (Robot Operating System)","description":"Introduction to ROS 2","source":"@site/docs/ros2/index.md","sourceDirName":"ros2","slug":"/ros2/","permalink":"/full-project/docs/ros2/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/index.md","tags":[],"version":"current","frontMatter":{"title":"ROS 2 (Robot Operating System)"}},{"id":"ros2/introduction/index","title":"Introduction to ROS 2","description":"The Robot Operating System (ROS) is a flexible framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex and robust robot behavior across a wide variety of robotic platforms.","source":"@site/docs/ros2/introduction/index.md","sourceDirName":"ros2/introduction","slug":"/ros2/introduction/","permalink":"/full-project/docs/ros2/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"Introduction to ROS 2"},"sidebar":"tutorialSidebar","previous":{"title":"2. ROS 2 (Robot Operating System)","permalink":"/full-project/docs/category/2-ros-2-robot-operating-system"},"next":{"title":"Core Architecture","permalink":"/full-project/docs/ros2/core-architecture/"}},{"id":"ros2/python-integration-rclpy/index","title":"Python Integration (rclpy)","description":"rclpy is the official Python client library for ROS 2. It provides a Pythonic interface to all the core ROS 2 concepts, allowing developers to write ROS 2 nodes, publishers, subscribers, services, and actions in Python.","source":"@site/docs/ros2/python-integration-rclpy/index.md","sourceDirName":"ros2/python-integration-rclpy","slug":"/ros2/python-integration-rclpy/","permalink":"/full-project/docs/ros2/python-integration-rclpy/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/python-integration-rclpy/index.md","tags":[],"version":"current","frontMatter":{"title":"Python Integration (rclpy)"},"sidebar":"tutorialSidebar","previous":{"title":"Communication Patterns","permalink":"/full-project/docs/ros2/communication-patterns/"},"next":{"title":"URDF (Unified Robot Description Format)","permalink":"/full-project/docs/ros2/urdf/"}},{"id":"ros2/urdf/index","title":"URDF (Unified Robot Description Format)","description":"The Unified Robot Description Format (URDF) is an XML format for describing the physical structure of a robot. It is a key component of the ROS ecosystem, used for modeling, simulation, and visualization.","source":"@site/docs/ros2/urdf/index.md","sourceDirName":"ros2/urdf","slug":"/ros2/urdf/","permalink":"/full-project/docs/ros2/urdf/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/ros2/urdf/index.md","tags":[],"version":"current","frontMatter":{"title":"URDF (Unified Robot Description Format)"},"sidebar":"tutorialSidebar","previous":{"title":"Python Integration (rclpy)","permalink":"/full-project/docs/ros2/python-integration-rclpy/"},"next":{"title":"Conclusion","permalink":"/full-project/docs/ros2/conclusion/"}},{"id":"simulation/conclusion/index","title":"Conclusion","description":"Simulation is a critical tool for modern robotics development. Platforms like Gazebo and Unity provide powerful and flexible environments for testing, training, and debugging robots. By understanding the strengths and weaknesses of different platforms and the principles of sensor simulation, you will be well-equipped to leverage simulation to accelerate your robotics projects.","source":"@site/docs/simulation/conclusion/index.md","sourceDirName":"simulation/conclusion","slug":"/simulation/conclusion/","permalink":"/full-project/docs/simulation/conclusion/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/conclusion/index.md","tags":[],"version":"current","frontMatter":{"title":"Conclusion"},"sidebar":"tutorialSidebar","previous":{"title":"Sensor Simulation","permalink":"/full-project/docs/simulation/sensor-simulation/"},"next":{"title":"5. Vision-Language-Action (VLA)","permalink":"/full-project/docs/category/5-vision-language-action-vla"}},{"id":"simulation/index","title":"Robot Simulation","description":"The Importance of Simulation in Robotics","source":"@site/docs/simulation/index.md","sourceDirName":"simulation","slug":"/simulation/","permalink":"/full-project/docs/simulation/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/index.md","tags":[],"version":"current","frontMatter":{"title":"Robot Simulation"}},{"id":"simulation/introduction/index","title":"The Importance of Simulation in Robotics","description":"Simulation is an indispensable tool in modern robotics development. It provides a virtual environment where robots can be tested, trained, and debugged without the risk of damaging expensive hardware or endangering humans. The ability to simulate a robot's behavior in a controlled and repeatable manner accelerates the development process and enables the exploration of complex scenarios that would be difficult or impossible to test in the real world.","source":"@site/docs/simulation/introduction/index.md","sourceDirName":"simulation/introduction","slug":"/simulation/introduction/","permalink":"/full-project/docs/simulation/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"The Importance of Simulation in Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"3. Robot Simulation","permalink":"/full-project/docs/category/3-robot-simulation"},"next":{"title":"Simulation Platforms: Gazebo vs. Unity","permalink":"/full-project/docs/simulation/platforms/"}},{"id":"simulation/platforms/index","title":"Simulation Platforms: Gazebo vs. Unity","description":"There are several robot simulation platforms available, each with its own strengths and weaknesses. Two of the most popular platforms in the robotics community are Gazebo and Unity.","source":"@site/docs/simulation/platforms/index.md","sourceDirName":"simulation/platforms","slug":"/simulation/platforms/","permalink":"/full-project/docs/simulation/platforms/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/platforms/index.md","tags":[],"version":"current","frontMatter":{"title":"Simulation Platforms: Gazebo vs. Unity"},"sidebar":"tutorialSidebar","previous":{"title":"The Importance of Simulation in Robotics","permalink":"/full-project/docs/simulation/introduction/"},"next":{"title":"Sensor Simulation","permalink":"/full-project/docs/simulation/sensor-simulation/"}},{"id":"simulation/sensor-simulation/index","title":"Sensor Simulation","description":"Accurate sensor simulation is crucial for developing and testing perception algorithms. Both Gazebo and Unity provide a wide range of sensor models.","source":"@site/docs/simulation/sensor-simulation/index.md","sourceDirName":"simulation/sensor-simulation","slug":"/simulation/sensor-simulation/","permalink":"/full-project/docs/simulation/sensor-simulation/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/simulation/sensor-simulation/index.md","tags":[],"version":"current","frontMatter":{"title":"Sensor Simulation"},"sidebar":"tutorialSidebar","previous":{"title":"Simulation Platforms: Gazebo vs. Unity","permalink":"/full-project/docs/simulation/platforms/"},"next":{"title":"Conclusion","permalink":"/full-project/docs/simulation/conclusion/"}},{"id":"vla/conclusion/index","title":"Conclusion","description":"Vision-Language-Action models are a rapidly evolving area of AI with the potential to revolutionize the way we interact with robots. By combining the power of perception, language, and action, we can create truly intelligent agents that can understand our world and our intentions. As these models continue to improve, we can expect to see a new generation of robots that are more capable, more versatile, and more collaborative than ever before.","source":"@site/docs/vla/conclusion/index.md","sourceDirName":"vla/conclusion","slug":"/vla/conclusion/","permalink":"/full-project/docs/vla/conclusion/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/conclusion/index.md","tags":[],"version":"current","frontMatter":{"title":"Conclusion"},"sidebar":"tutorialSidebar","previous":{"title":"The Role of Large Language Models in Robotics","permalink":"/full-project/docs/vla/llm-role"},"next":{"title":"6. Humanoid Robotics","permalink":"/full-project/docs/category/6-humanoid-robotics"}},{"id":"vla/index","title":"Vision-Language-Action (VLA)","description":"The Convergence of Perception, Language, and Action","source":"@site/docs/vla/index.md","sourceDirName":"vla","slug":"/vla/","permalink":"/full-project/docs/vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/index.md","tags":[],"version":"current","frontMatter":{"title":"Vision-Language-Action (VLA)"}},{"id":"vla/introduction/index","title":"The Convergence of Perception, Language, and Action","description":"Vision-Language-Action (VLA) models represent the cutting edge of AI, bringing together three distinct fields to create truly intelligent agents that can understand and interact with the world in a human-like way. VLAs are the key to building robots that can follow natural language commands, learn new tasks from observation, and collaborate with humans in a shared environment.","source":"@site/docs/vla/introduction/index.md","sourceDirName":"vla/introduction","slug":"/vla/introduction/","permalink":"/full-project/docs/vla/introduction/","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/introduction/index.md","tags":[],"version":"current","frontMatter":{"title":"The Convergence of Perception, Language, and Action"},"sidebar":"tutorialSidebar","previous":{"title":"5. Vision-Language-Action (VLA)","permalink":"/full-project/docs/category/5-vision-language-action-vla"},"next":{"title":"The VLA Pipeline","permalink":"/full-project/docs/vla/pipeline"}},{"id":"vla/llm-role","title":"The Role of Large Language Models in Robotics","description":"The recent advances in Large Language Models have had a transformative impact on the field of robotics. LLMs are not just powerful language models; they are also powerful reasoning engines that can be used to solve a wide range of robotics problems.","source":"@site/docs/vla/llm-role.md","sourceDirName":"vla","slug":"/vla/llm-role","permalink":"/full-project/docs/vla/llm-role","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/llm-role.md","tags":[],"version":"current","frontMatter":{"title":"The Role of Large Language Models in Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"The VLA Pipeline","permalink":"/full-project/docs/vla/pipeline"},"next":{"title":"Conclusion","permalink":"/full-project/docs/vla/conclusion/"}},{"id":"vla/pipeline","title":"The VLA Pipeline","description":"A typical VLA pipeline consists of three main stages:","source":"@site/docs/vla/pipeline.md","sourceDirName":"vla","slug":"/vla/pipeline","permalink":"/full-project/docs/vla/pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/pipeline.md","tags":[],"version":"current","frontMatter":{"title":"The VLA Pipeline"},"sidebar":"tutorialSidebar","previous":{"title":"The Convergence of Perception, Language, and Action","permalink":"/full-project/docs/vla/introduction/"},"next":{"title":"The Role of Large Language Models in Robotics","permalink":"/full-project/docs/vla/llm-role"}}],"drafts":[],"sidebars":{"tutorialSidebar":[{"type":"doc","id":"intro"},{"type":"category","label":"1. Physical AI Fundamentals","link":{"type":"generated-index","title":"Physical AI Fundamentals","slug":"/category/physical-ai","permalink":"/full-project/docs/category/physical-ai"},"items":[{"type":"doc","id":"physical-ai/introduction/index"},{"type":"doc","id":"physical-ai/core-principles/index"},{"type":"doc","id":"physical-ai/digital-to-physical-transition/index"},{"type":"doc","id":"physical-ai/conclusion/index"}],"collapsible":true,"collapsed":true},{"type":"category","label":"2. ROS 2 (Robot Operating System)","link":{"type":"generated-index","title":"ROS 2 (Robot Operating System)","slug":"/category/2-ros-2-robot-operating-system","permalink":"/full-project/docs/category/2-ros-2-robot-operating-system"},"items":[{"type":"doc","id":"ros2/introduction/index"},{"type":"doc","id":"ros2/core-architecture/index"},{"type":"doc","id":"ros2/communication-patterns/index"},{"type":"doc","id":"ros2/python-integration-rclpy/index"},{"type":"doc","id":"ros2/urdf/index"},{"type":"doc","id":"ros2/conclusion/index"}],"collapsible":true,"collapsed":true},{"type":"category","label":"3. Robot Simulation","link":{"type":"generated-index","title":"Robot Simulation","slug":"/category/3-robot-simulation","permalink":"/full-project/docs/category/3-robot-simulation"},"items":[{"type":"doc","id":"simulation/introduction/index"},{"type":"doc","id":"simulation/platforms/index"},{"type":"doc","id":"simulation/sensor-simulation/index"},{"type":"doc","id":"simulation/conclusion/index"}],"collapsible":true,"collapsed":true},{"type":"category","label":"5. Vision-Language-Action (VLA)","link":{"type":"generated-index","title":"Vision-Language-Action (VLA)","slug":"/category/5-vision-language-action-vla","permalink":"/full-project/docs/category/5-vision-language-action-vla"},"items":[{"type":"doc","id":"vla/introduction/index"},{"type":"doc","id":"vla/pipeline"},{"type":"doc","id":"vla/llm-role"},{"type":"doc","id":"vla/conclusion/index"}],"collapsible":true,"collapsed":true},{"type":"category","label":"6. Humanoid Robotics","link":{"type":"generated-index","title":"Humanoid Robotics","slug":"/category/6-humanoid-robotics","permalink":"/full-project/docs/category/6-humanoid-robotics"},"items":[{"type":"doc","id":"humanoid-robotics/introduction/index"},{"type":"doc","id":"humanoid-robotics/kinematics-dynamics"},{"type":"doc","id":"humanoid-robotics/bipedal-locomotion"},{"type":"doc","id":"humanoid-robotics/manipulation-grasping"},{"type":"doc","id":"humanoid-robotics/human-robot-interaction"},{"type":"doc","id":"humanoid-robotics/conclusion/index"}],"collapsible":true,"collapsed":true},{"type":"category","label":"7. Hardware Setup","link":{"type":"generated-index","title":"Hardware Setup","slug":"/category/7-hardware-setup","permalink":"/full-project/docs/category/7-hardware-setup"},"items":[{"type":"doc","id":"hardware/introduction/index"},{"type":"doc","id":"hardware/workstation-requirements"},{"type":"doc","id":"hardware/edge-computing-jetson"},{"type":"doc","id":"hardware/sensors-actuators"},{"type":"doc","id":"hardware/lab-infrastructure"},{"type":"doc","id":"hardware/conclusion/index"}],"collapsible":true,"collapsed":true}]}}]}},"docusaurus-plugin-content-pages":{"default":[{"type":"jsx","permalink":"/full-project/","source":"@site/src/pages/index.tsx"},{"type":"jsx","permalink":"/full-project/login","source":"@site/src/pages/login.tsx"},{"type":"jsx","permalink":"/full-project/signup","source":"@site/src/pages/signup.tsx"}]},"docusaurus-plugin-debug":{},"docusaurus-plugin-svgr":{},"docusaurus-theme-classic":{},"docusaurus-bootstrap-plugin":{},"docusaurus-mdx-fallback-plugin":{}}}